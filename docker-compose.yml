services:
  # -----------------------------
  # 2. FalkorDB Graph Database (Redis-based)
  # -----------------------------
  falkordb:
    image: falkordb/falkordb:latest
    container_name: llm_knowledge_graph_falkordb
    ports:
      - "6379:6379"   # Redis protocol
      - "3000:3000"   # FalkorDB Browser UI (optional)
    environment:
      - FALKORDB_ARGS=--maxclients 512
    volumes:
      - ./falkordb_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # -----------------------------
  # 3. Your Python LLM App
  # -----------------------------
  llm-app:
    build: .
    container_name: llm_knowledge_graph_app
    ports:
      - "8000:8000"  # Chainlit web interface
    depends_on:
      falkordb:
        condition: service_healthy
    environment:
      # FalkorDB Configuration
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - FALKORDB_GRAPH_NAME=wikidata
      # API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY} # Reads from your local shell or .env file
      - GOOGLE_API_KEY=${GOOGLE_API_KEY} # For Gemini API
    volumes:
      - .:/app # Live reload: changes in your folder reflect inside container
    # Keep the container running indefinitely so you can enter the CLI
    command: tail -f /dev/null
