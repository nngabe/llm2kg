"""
Golden Dataset Reviewer.

CLI tool for human review of generated test cases.
"""

import json
import logging
from pathlib import Path
from typing import Optional

from .schema import GoldenTestCase, GoldenDataset

logger = logging.getLogger(__name__)


class GoldenDatasetReviewer:
    """Interactive CLI for reviewing golden test cases."""

    def __init__(self, dataset: GoldenDataset, reviewer_name: str = "anonymous"):
        self.dataset = dataset
        self.reviewer_name = reviewer_name
        self._current_index = 0

    def _display_test_case(self, tc: GoldenTestCase) -> None:
        """Display a test case for review."""
        print("\n" + "=" * 60)
        print(f"TEST CASE: {tc.id}")
        print("=" * 60)

        print(f"\nType: {tc.type} | Difficulty: {tc.difficulty} | Source: {tc.source}")
        print(f"Status: {tc.review_status}")

        if tc.generated_by:
            print(f"Generated by: {tc.generated_by}")

        print(f"\n{'─' * 40}")
        print("QUESTION:")
        print(f"  {tc.question}")

        print(f"\n{'─' * 40}")
        print("EXPECTED ANSWER:")
        print(f"  {tc.expected_answer or 'Not specified'}")

        if tc.expected_entities:
            print(f"\n{'─' * 40}")
            print("EXPECTED ENTITIES:")
            for entity in tc.expected_entities:
                print(f"  - {entity}")

        if tc.expected_relationships:
            print(f"\n{'─' * 40}")
            print("EXPECTED RELATIONSHIPS:")
            for rel in tc.expected_relationships:
                if isinstance(rel, dict):
                    print(f"  - {rel.get('source')} --[{rel.get('type')}]--> {rel.get('target')}")
                else:
                    print(f"  - {rel}")

        if tc.optimal_tool_sequence:
            print(f"\n{'─' * 40}")
            print("OPTIMAL TOOL SEQUENCE:")
            for i, tool in enumerate(tc.optimal_tool_sequence, 1):
                print(f"  {i}. {tool}")

        if tc.minimum_steps:
            print(f"\nMinimum steps: {tc.minimum_steps}")

        if tc.should_reject:
            print(f"\n{'─' * 40}")
            print("REJECTION TEST")
            print(f"  Should reject: Yes")
            print(f"  Reason: {tc.rejection_reason or 'Not specified'}")

        if tc.ground_truth_context:
            print(f"\n{'─' * 40}")
            print("GROUND TRUTH CONTEXT:")
            for ctx in tc.ground_truth_context[:3]:
                print(f"  - {ctx[:100]}...")

        if tc.metadata.get("reasoning"):
            print(f"\n{'─' * 40}")
            print("GENERATION REASONING:")
            print(f"  {tc.metadata['reasoning']}")

        print("\n" + "=" * 60)

    def _get_action(self) -> str:
        """Get user action for current test case."""
        print("\nActions:")
        print("  [a] Approve")
        print("  [r] Reject")
        print("  [e] Edit")
        print("  [s] Skip")
        print("  [n] Next")
        print("  [p] Previous")
        print("  [q] Quit and save")
        print("  [x] Quit without saving")

        while True:
            action = input("\nAction: ").strip().lower()
            if action in ["a", "r", "e", "s", "n", "p", "q", "x"]:
                return action
            print("Invalid action. Please try again.")

    def _edit_test_case(self, tc: GoldenTestCase) -> None:
        """Interactive editing of a test case."""
        print("\nEditing test case (press Enter to keep current value)")

        # Edit question
        new_question = input(f"Question [{tc.question[:50]}...]: ").strip()
        if new_question:
            tc.question = new_question

        # Edit expected answer
        new_answer = input(f"Expected answer [{(tc.expected_answer or 'None')[:50]}...]: ").strip()
        if new_answer:
            tc.expected_answer = new_answer

        # Edit difficulty
        new_diff = input(f"Difficulty [easy/medium/hard] ({tc.difficulty}): ").strip().lower()
        if new_diff in ["easy", "medium", "hard"]:
            tc.difficulty = new_diff

        # Edit type
        new_type = input(f"Type [retrieval/agentic/integrity/generation/rejection] ({tc.type}): ").strip().lower()
        if new_type in ["retrieval", "agentic", "integrity", "generation", "rejection", "all"]:
            tc.type = new_type

        # Edit should_reject
        if tc.type == "rejection" or tc.should_reject:
            reject = input(f"Should reject [y/n] ({'y' if tc.should_reject else 'n'}): ").strip().lower()
            if reject == "y":
                tc.should_reject = True
                reason = input(f"Rejection reason: ").strip()
                if reason:
                    tc.rejection_reason = reason
            elif reject == "n":
                tc.should_reject = False
                tc.rejection_reason = None

        # Edit expected entities
        if input("Edit expected entities? [y/n]: ").strip().lower() == "y":
            print(f"Current: {tc.expected_entities}")
            entities = input("New entities (comma-separated): ").strip()
            if entities:
                tc.expected_entities = [e.strip() for e in entities.split(",")]

        # Edit minimum steps
        steps = input(f"Minimum steps ({tc.minimum_steps or 'None'}): ").strip()
        if steps.isdigit():
            tc.minimum_steps = int(steps)

        tc.source = "hybrid"  # Mark as human-edited
        print("\nTest case updated!")

    def review_all(self) -> None:
        """Review all pending test cases."""
        pending = self.dataset.get_pending_review()

        if not pending:
            print("No test cases pending review!")
            return

        print(f"\n{len(pending)} test cases pending review")
        print("Starting review session...\n")

        self._current_index = 0

        while self._current_index < len(pending):
            tc = pending[self._current_index]
            self._display_test_case(tc)

            action = self._get_action()

            if action == "a":
                notes = input("Approval notes (optional): ").strip()
                tc.approve(self.reviewer_name, notes or None)
                print(f"Approved: {tc.id}")
                self._current_index += 1

            elif action == "r":
                reason = input("Rejection reason: ").strip()
                if not reason:
                    print("Rejection reason required!")
                    continue
                tc.reject(self.reviewer_name, reason)
                print(f"Rejected: {tc.id}")
                self._current_index += 1

            elif action == "e":
                self._edit_test_case(tc)

            elif action == "s":
                print(f"Skipped: {tc.id}")
                self._current_index += 1

            elif action == "n":
                self._current_index += 1

            elif action == "p":
                self._current_index = max(0, self._current_index - 1)

            elif action == "q":
                self._save_progress()
                print("Review session saved and ended.")
                return

            elif action == "x":
                confirm = input("Quit without saving? [y/n]: ").strip().lower()
                if confirm == "y":
                    print("Review session ended without saving.")
                    return

        print("\nAll test cases reviewed!")
        self._save_progress()

    def review_single(self, test_case_id: str) -> None:
        """Review a single test case by ID."""
        tc = self.dataset.get_test_case(test_case_id)

        if not tc:
            print(f"Test case not found: {test_case_id}")
            return

        self._display_test_case(tc)
        action = self._get_action()

        if action == "a":
            notes = input("Approval notes (optional): ").strip()
            tc.approve(self.reviewer_name, notes or None)
            print(f"Approved: {tc.id}")
        elif action == "r":
            reason = input("Rejection reason: ").strip()
            if reason:
                tc.reject(self.reviewer_name, reason)
                print(f"Rejected: {tc.id}")
        elif action == "e":
            self._edit_test_case(tc)

        self._save_progress()

    def _save_progress(self) -> None:
        """Save current dataset state."""
        # Save to a review progress file
        progress_path = Path(f"benchmarks/enterprise_eval/golden/datasets/{self.dataset.name}_review.json")
        self.dataset.save(progress_path)
        print(f"Progress saved to {progress_path}")

    def show_statistics(self) -> None:
        """Display dataset statistics."""
        stats = self.dataset.get_statistics()

        print("\n" + "=" * 40)
        print("DATASET STATISTICS")
        print("=" * 40)
        print(f"Total test cases: {stats['total']}")
        print(f"Approved: {stats.get('approved_count', 0)}")
        print(f"Pending: {stats.get('pending_count', 0)}")

        print("\nBy Type:")
        for t, count in stats.get("by_type", {}).items():
            print(f"  {t}: {count}")

        print("\nBy Difficulty:")
        for d, count in stats.get("by_difficulty", {}).items():
            print(f"  {d}: {count}")

        print("\nBy Status:")
        for s, count in stats.get("by_status", {}).items():
            print(f"  {s}: {count}")

        print("\nBy Source:")
        for s, count in stats.get("by_source", {}).items():
            print(f"  {s}: {count}")

        print(f"\nRejection tests: {stats.get('rejection_test_count', 0)}")


def run_review_cli(dataset_path: str, reviewer_name: str = "anonymous"):
    """Run the review CLI.

    Args:
        dataset_path: Path to the dataset JSON file
        reviewer_name: Name of the reviewer
    """
    try:
        dataset = GoldenDataset.load(Path(dataset_path))
    except FileNotFoundError:
        print(f"Dataset not found: {dataset_path}")
        return

    reviewer = GoldenDatasetReviewer(dataset, reviewer_name)

    print(f"\nLoaded dataset: {dataset.name}")
    reviewer.show_statistics()

    print("\nOptions:")
    print("  [1] Review all pending")
    print("  [2] Review specific test case")
    print("  [3] Show statistics")
    print("  [4] Export approved")
    print("  [q] Quit")

    while True:
        choice = input("\nChoice: ").strip()

        if choice == "1":
            reviewer.review_all()
        elif choice == "2":
            tc_id = input("Test case ID: ").strip()
            reviewer.review_single(tc_id)
        elif choice == "3":
            reviewer.show_statistics()
        elif choice == "4":
            approved = dataset.get_approved()
            export_path = Path(f"benchmarks/enterprise_eval/golden/datasets/{dataset.name}_approved.json")
            approved_dataset = GoldenDataset(
                name=f"{dataset.name}_approved",
                description="Approved test cases only",
            )
            for tc in approved:
                approved_dataset.add_test_case(tc)
            approved_dataset.save(export_path)
            print(f"Exported {len(approved)} approved test cases to {export_path}")
        elif choice.lower() == "q":
            break
        else:
            print("Invalid choice")
